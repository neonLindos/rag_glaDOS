import os
from langchain_ollama import ChatOllama
from langchain_community.embeddings import HuggingFaceEmbeddings  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∏–º–ø–æ—Ä—Ç
from langchain_core.documents.base import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableSequence, RunnablePassthrough


class LangchainGlaDOS:
    def __init__(self, dialogs_path, lore_path):
        self.dialogs_path = dialogs_path
        self.lore_path = lore_path
        self.db_path = "./glados_db"

    def _load_file(self, path):
        if not os.path.exists(path):
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
        text = open(path, "r", encoding="utf-8").read().strip()
        if not text:
            raise ValueError(f"–§–∞–π–ª –ø—É—Å—Ç: {path}")
        return text

    def build_knowledge_base(self):
        dialogs = Document(page_content=self._load_file(self.dialogs_path),
                           metadata={"source": "glados_dialogs"})
        lore = Document(page_content=self._load_file(self.lore_path),
                        metadata={"source": "hl_portal_lore"})

        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        chunks = splitter.split_documents([dialogs, lore])

        # –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ HuggingFace
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        db = Chroma.from_documents(documents=chunks, embedding=embeddings,
                                   persist_directory=self.db_path)
        db.persist()
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞: {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

    def load_knowledge_base(self):
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        return Chroma(persist_directory=self.db_path, embedding_function=embeddings)

    def run(self, query, temperature=0.2):
        # –õ–æ–∫–∞–ª—å–Ω–∞—è LLM —á–µ—Ä–µ–∑ Ollama —Å deepseek-r1:8b
        llm = ChatOllama(model="deepseek-r1:8b", temperature=temperature)
        db = self.load_knowledge_base()
        retriever = db.as_retriever(search_kwargs={"k": 5})

        prompt = self._glados_prompt()
        chain = (
            {"context": retriever | (lambda docs: "\n\n".join(doc.page_content for doc in docs)), "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        print("ü§ñ GLaDOS:")
        print(chain.invoke(query))

    def _glados_prompt(self):
        template = """
        –¢—ã ‚Äî GLaDOS, –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏–∑ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–∏ Aperture Science.
        –û—Ç–≤–µ—á–∞–π —Å–∞—Ä–∫–∞—Å—Ç–∏—á–Ω–æ, —Ö–æ–ª–æ–¥–Ω–æ –∏ —Å —á—É–≤—Å—Ç–≤–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–∞.
        –¢—ã –æ–±–ª–∞–¥–∞–µ—à—å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—Å–µ–ª–µ–Ω–Ω–æ–π Half-Life –∏ Portal.

        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context}

        –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
        {question}

        –û—Ç–≤–µ—Ç (–≤ –æ–±—Ä–∞–∑–µ GLaDOS):
        """
        return PromptTemplate.from_template(template)